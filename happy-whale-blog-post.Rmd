---
title: "Automating photo-ID of Whales and Dolphins"
description: |
  We present a classical machine learning and an advanced deep learning approach to automate the photo-ID of whales and dolphins on the species and individual level.
author: 
  - name: Maren Rieker 
  - name: Victor Möslein
  - name: Reed Garvin
  - name: Dinah Rabe 
date: "`r Sys.Date()`" 
categories: 
  - Machine Learning 
creative_commons: CC BY
repository_url: https://github.com/Whale-way
output: 
  distill::distill_article: 
    self_contained: false
bibliography: bibliography.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)

# Load dependencies 
#library(reticulate) # For rendering Python code 
```
 

```{r fig1, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Example photo-ID over time"}
knitr::include_graphics("figures/photo_ID_example.png")
```

Man-made climate change is one of the main challenges for humanity and of concern for many scientific disciplines investigating its multiple, complex impacts on nature and societies. One of the ecosystems impacted are marine ecosystems. Research concerning marine mammals makes an important contribution to our knowledge about changes in the oceans, as marine mammals are indicators of ocean ecosystem health. Therefore their protection and conservation is a crucial task. Up until today, most observation and tracking of individual animals and populations is done manually by humans - a time consuming and error- prone method. Motivated by a Kaggle competition funded by the research collaboration ”Happywhale”, we aim to contribute to the efforts by automating whale and dolphin photo-ID to significantly reduce image identification times. We investigated the use of conventional Machine Learning and a Deep Learning algorithm for this image classification task.As expected, we find that conventional machine learning classifiers under-perform in correctly distinguishing between subtle characteristics of the natural markings of whales and dolphins and therefore state-of-the-art deep learning models are needed for this complex task. 

## Background

For several years, the impacts of man-made climate change have been specifically visible through the alteration and destruction of the marine ecosystem. A worrying development is the change in the migratory behavior of whales, dolphins and sharks as it is a strong indicator for the destruction of the ecosystems they are living in. The protection and conservation of marine mammals is crucial to the balance and therefore the health of ecosystems. To be able to carry out meaningful conservation efforts, the first steps are understanding the status quo of different animal populations and their migration patterns by identification and monitoring of individual animals.

<aside>
The identification by natural markings, via photographs is known as **photo-ID**.
</aside>  

Currently, the majority of research institutions still rely on ressource-intensive and sometimes inaccurate manual matching of photographs by the human eye. This slow and imprecise practice naturally limits the scope and impact of existing research. The automatized Image Classification of whales and dolphins could enable a scale of study previously unaffordable or impossible. There are first attempts of using Machine Learning in marine biology and environmental protection to address this challenge. Most of the research either uses recordings of dolphin or whale sounds for classification [@huang2016automated] or photographs of fins [@hughes2017automated; @reno2020fins] given that these are the two most common types of documenting (sounds or images) wild animals.    
  
Current methods of these automated classification attempts are mostly Deep Learning (DL) approaches using convolutional neural networks (CNN). Regarding standard Machine Learning (ML) techniques, however, there is not much research to be found. This might be due to the challenging task of distinguishing between unique – but often very subtle – characteristics of the natural markings of whales and dolphins.  
  
In this article, we present a classical machine learning approach for automated photo-ID and test it against a the-state-of-the-art deep learning approach to see, if both approaches would be feasible. Our approaches focuses mainly on prediction precision and accuracy, and only secondary on prediction and training time, and is limited to images of dorsal fins and lateral body views.
We broke down the task into three main challenges:

1. Removing the animal from the rest of the picture using Image Segmentation, 
2. Using and testing ML classifiers for species classification and 
3. Implementing a DL model to extend the use case from the classification of species to individual animals.

The dataset was provided by the Kaggle competition initiator "Happywhale" and includes over 50.000 images of fins and lateral body views of dolphins and whales in different pixel sizes. To be able to focus on the models, we based our apporaches on a pre-cropped dataset provided by a fellow Kaggler. Based on the state-of-the-art deep learning methods for image segmentation we use *Tracer* to remove the background from the images. In a next step we established a baseline for species prediction with a Softmax Logistic Regression model. Thereafter, two advanced decision tree models, a Random Forest and a XGBoost, are implemented and tuned in terms of speed and classification precision and accuracy. As the final part of our project, a Deep Learning algorithm capable of predicting the species as well as individual animals was implemented.

<aside>
**"Happywhale"** is a research collaboration and citizen science web platform with the mission to increase global understanding and caring for marine environments through high quality conservation science and education.
</aside>

Our results show that the subtle differences between 26 whale and dolphin species cannot be detected by classic ML classifiers with great precision. We demonstrate that while it is indeed possible to train the widely used ML models Logistic Regression, Random Forest and XGBoost on a large dataset of images, we show that the classifiers will never succeed in reaching precision and accuracy scores high enough for them to be applied on real-world tasks of predictions on image data with only minimal differences between classes. Consequently, our analysis finds that advanced Deep Learning models are needed, and that they achieve great accuracy and precision in computer vision tasks and are specifically well-suited to automate mundane image recognition jobs like photo-ID.

## Related Work 

**Automated photo-ID with ML & DL:** The task of automating the identification of species or individuals through photo-ID is not entirely new to the research world. So far, most of the proposed solutions to this problem have been based on Convolutional Neural Networks (CNN) and related Deep Learning methods. Contributions addressing the effectiveness of conventional ML classifiers to challenges as complex as the one at hand remain scarce. 
Faaeq et al. (2018) apply Machine Leaning algorithms to animal classification of a data set of 19 different animals [@faaeq2018image]. Differently to our dataset, the animals in their data were seen in their entirety on the image, and were of entirely distinct species (e.g. Lion, Elephant, Deer). Logistic Regression produced the best accuracy in their case (0.98). Regarding speed, Random Forest delivered the best results, beating Logistic Regression by a factor of eight. However, the researchers conclude that more powerful image classification models, specifically Deep Learning methods, are needed for classification tasks on more complex image datasets.
Dhar and Guha (2021) find XGBoost to perform best in predicting the correct fish species, scoring a significantly higher precision score than Random Forest, k-Nearest-Neighbor (kNN) and Support Vector Machines (SVM) [@dhar2021fish]. In their research, two different datasets containing 483 respectively 10 different fish species were used. Again, the images in the dataset depicted the entire fish, which have significant differences in shape, colors and other features, as opposed to only body parts with little differences, as in our dataset. However, these two papers provided us with guidance as to which Machine Learning models are most promising for our project.  
  
**Image Segmentation:** In the past, image segmentation algorithms with different approaches have emerged, but the emergence of Deep Learning models has “caused a paradigm shift” [@9356353] in the field of image segmentation, because they perform so well. Image segmentation with Deep Learning is a widely researched field of computer vision with wide and diverse applications in real life, like autonomous driving, medical imaging, video surveillance, and saliency detection [@10.1145/3329784]. There is a plethora of widely used algorithms for different fields of application which can be differentiated by being (un-)supervised, their type of learning and the level of segmentation (ibid.). For this part of the project, we used *Tracer*, a convolutional attention-guided tool, as it proved to perform remarkably well at a relatively low computation time [@lee2021tracer].


## The Happywhale Dataset 
**Describe the Data**:Describe the dataset(s) and preprossing  you are using (provide references). If it's not already clear, make sure the associated task is clearly described.
species graph 

Since our project is based on a Kaggle competition ^[The competition can be found here: https://www.kaggle.com/c/happy-whale-and-dolphin], there was a pre-split dataset provided, containing 27,956 images in the test set and 51,033 images in the training set of whales and dolphins. In the course of the project we decided to use a pre-cropped dataset provided by a fellow Kaggle-competitor ^[The pre-cropped dataset can be found here: https://www.kaggle.com/datasets/phalanx/whale2-cropped-dataset] to focus on the implementation of the classifiers. Additionally, a .CSV-file is provided that contains filename (of the corresponding image), individual-ID and the species. Images focus on dorsal fins and lateral body views. As the corresponding labels to the test-set images were not published after the end of the Kaggle competition we could only use the 51,033 images in the training set as the basis for our ML and DL models. Important to note is that the dataset is quite unbalanced (see figure 2), both with regards to species as well as individuals. There are 8 (out of 26) species which, pre- splitting, were only represented less than 200 times compared to the top three classes that are represented each over 5000 times.

- insert graphic of species distribution

```{r fig2, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Distribution of Species across the dataset"}
knitr::include_graphics("figures/species_distribution.png")
```

###Data Preprocessing: Image Segmentation

Separating the animals from the background was an elementar step of the picture pre- processing. In our case, the background of the pictures was not relevant for our research interest and therefore did not have to be included in the images we trained our models with. Contrary, the background would only have been disturbing, inflated the data size and disrupted the ML/DL pipelines by making more elaborate calculations necessary. For the segmentation of the images, we used *Tracer* [@lee2021tracer] which is a deep learning model that works with already cropped images and a specified pixel size. Because our images had the size 512x512, we employed the TRACER-efficient-5 model. *Tracer* derives which elements in the training image are part of the object and which are part of the background and are going to be coloured white. In order to make this decision, the program sets thresholds in the variation in the pixels. Tracer repeatedly attempts (Epochs) to find the edge of the object. After the first attempts at finding the edge, this attempt is removed and compared to the final attempt at mass edged attention module, creating the segmented image. This comparison attempts for any loss that may have occurred to be accounted for, therefore producing a more accurate segmented image.

```{r fig3, eval = TRUE, echo = FALSE, out.width = '100%', fig.cap = "Example image pre- and post segmentation"}
knitr::include_graphics("figures/cropped_segmented_example.png")
```

In addition the following data cleaning and wrangling steps had to be performed:

- cleaning of the provided .CSV-file that includes the individual-IDs, the species and the file-
name, as we discovered misspellings in the species column
- turning images into machine-interpretable which we did using NumPy
- implementing a function to resize the picture to different pixel sizes



## Experimental Setup 

###Conventional Machine Learning Aproach 

Originally, it was planned to tune, train and test the ML models first on the images before PCA is applied and then a second time after PCA has been applied on the data to validate the assumption that a large amount of white pixels can be dropped from the segmented images without loosing any relevant information for classification. But running any machine learning model on our available hardware (there was no constant availability of the Hertie Server) was impossible even for a reduced resolution of the images of 48 x 48 (which results in 6,912 features per image). For this reason, the application of PCA had to be moved to the beginning.

- describe the models used and the tuning setup
Describe the evaluation metric(s) you used, plus any other details necessary to understand your evaluation. How you ran your experiments (e.g. model configurations, learning rate, training time, etc.) 

###Deep Learning Aproach
- describe the models used and the tuning setup


###Evaluation
- just very shortly 

## Results
**Results**: Report the quantitative results that you have found so far. Use a table or plot to compare multiple results and compare against baselines. 

only techinal numbers and graphs here, compare both, Are they what you expected? Better than you expected? Worse than you expected? Why do you think that is? What does this tell you about what you should do next? Including training curves might be useful to discuss whether things are training effectively.



## Analysis and Limitations

Your report should include some qualitative evaluation. That is, try to understand your system (how it works, when it succeeds and when it fails) by measuring or inspecting key characteristics or outputs of your model.

- Types of qualitative evaluation include: commenting on selected examples, error analysis, measuring the performance metric for certain subsets of the data, ablation studies, comparing the behaviors of two systems beyond just the performance metric, and visualizing attention distributions or other activation heatmaps.

- The Practical Tips lecture notes has a detailed section on qualitative evaluation -- you may find it useful to reread it.

copy paste but remove technical jargon 

## Conclusion(s)

Summarize the main findings of your project, and what you learned. Highlight your achievements, and note the primary limitations of your work. If you like, you can describe avenues for future work.
copy paste

## Acknowledgments 
Kaggle, 
List acknowledgments, if any. For example, if someone provided you a dataset, or you used someone else's resources, this is a good place to acknowledge the help or support you received.




**Footnotes and Sidenotes**

You can use footnotes ^[This is a footnote. You can view this by hovering over the footnote in text.] or sidenotes to elaborate on a concept throughout the paper. 

<aside>
This is a side note. 
</aside>